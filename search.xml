<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>学习笔记 - Google分布式文件系统</title>
      <link href="/2020/07/01/2020-07-01-google-file-system/"/>
      <url>/2020/07/01/2020-07-01-google-file-system/</url>
      
        <content type="html"><![CDATA[<p>主要参考：CMU <a href="http://www.andrew.cmu.edu/course/15-440/" target="_blank" rel="noopener">15-640</a>，以及<a href="https://dl.acm.org/doi/pdf/10.1145/945445.945450" target="_blank" rel="noopener">原论文</a></p><h3 id="1-GFS的硬件基础"><a href="#1-GFS的硬件基础" class="headerlink" title="1. GFS的硬件基础"></a>1. GFS的硬件基础</h3><ul><li>大型数据中心，大量的服务器和硬盘以rack为单位，rack之间主要以树型拓扑连接</li><li>主要特点<ul><li>同一rack之间的服务器交流带宽高、延迟低、拥塞情况良好</li><li>不同rack之间的服务器交流的有效带宽有限、延迟稍高、拥塞情况随着层级地上升而加重</li><li>由于一个数据中心内含有的服务器和硬盘数量巨大（数以千计的服务器、数以百万计的硬盘），硬件错误、崩溃是常有的事情</li></ul></li></ul><h3 id="2-GFS的使用场景假设"><a href="#2-GFS的使用场景假设" class="headerlink" title="2. GFS的使用场景假设"></a>2. GFS的使用场景假设</h3><ul><li>存储大文件（&gt;=100MB），小文件也支持，但是不会做优化</li><li>两种读操作<ul><li>较大的顺序读取，例如一次读1MB</li><li>较小的随机读取，例如一次读1KB，同样不会做优化</li></ul></li><li>两种写操作<ul><li>较大的顺序写操作，并且多以record append（找不到合适的中文翻译，下文简称append）为主</li><li>较小的随机写操作，同样不会做优化</li></ul></li><li>并发写操作也主要以record append操作为主，这也是整个系统的主要优化方向</li></ul><p>注：在后面我们会看到record append操作所具有的并发优势。</p><h3 id="3-GFS的设计目标"><a href="#3-GFS的设计目标" class="headerlink" title="3. GFS的设计目标"></a>3. GFS的设计目标</h3><ol><li>保持数据和整体系统的高可用性</li><li>容错机制能够对用户透明</li><li>为了实现系统各部件的同步所需要的overhead尽量小</li><li>高效利用庞大的服务器和硬盘资源</li><li>相比于低延迟，更倾向于维持高吞吐水平</li></ol><h3 id="4-GFS的基本框架和主要部件"><a href="#4-GFS的基本框架和主要部件" class="headerlink" title="4. GFS的基本框架和主要部件"></a>4. GFS的基本框架和主要部件</h3><p>一个集群（cluster）内包含</p><ul><li>一个主服务器（master server）</li><li>很多分块服务器（chunk server）</li></ul><p><img src="/images/image-20200628211731740.png" alt="GFS整体框架图"></p><p>从上图中我们可以得到以下信息</p><ul><li>主服务器存储整个文件系统的名称空间以及每一个文件的分块信息</li><li>分块服务器是实际存放文件（以块为单位）的机器</li><li>客户应用会从主服务器获取有关目标文件的分块信息</li><li>在获取分块信息后，客户应用会直接向分块服务器发起数据请求（这一过程与HTTP重定向类似）</li></ul><h4 id="4-1-主服务器"><a href="#4-1-主服务器" class="headerlink" title="4.1 主服务器"></a>4.1 主服务器</h4><ol><li><p>主服务器负责管理该文件系统中的所有文件的元信息（metadata），这些元信息包括</p><ul><li><p>名称空间（namespace），即文件路径</p></li><li><p>访问控制信息，即文件权限</p></li><li><p>文件分块信息，包括块编号和块句柄（chunk handle）</p></li><li><p>实际存储分块数据的分块服务器信息（包括主分块服务器和所有副分块服务器）</p></li></ul></li><li><p>主服务器负责分块数据的迁移</p></li></ol><ul><li>为什么需要进行迁移？</li></ul><ol start="3"><li>主服务器负责保持整个文件系统的一致性管理</li><li>主服务器负责垃圾回收所有孤儿分块数据</li></ol><h4 id="4-2-分块服务器"><a href="#4-2-分块服务器" class="headerlink" title="4.2 分块服务器"></a>4.2 分块服务器</h4><ol><li>分块服务器负责实际存储文件块，以及文件块对应的版本编号和校验和</li><li>分块服务器职能单一，只需要对客户应用请求所处回应以及定期向主服务器汇报（heartbeat）</li><li>客户应用发出的请求包含对应文件块的块句柄和所请求的数据范围（byte range）</li><li>分块服务器中的每一个文件块都会存在三个备份（默认值），三个会在不同的分块服务器上，通常其中的两个分块服务器会处在同一个rack中，另一个会处在不同的rack中。这样做是既考虑到了分块服务器间通信效率，又考虑到了容灾性能</li><li>分块服务器侧不会缓存数据（除了Linux系统自带的文件缓存机制），同样，客户应用侧也不会缓存数据，客户应用侧只会缓存文件元信息</li></ol><h4 id="4-3-客户应用"><a href="#4-3-客户应用" class="headerlink" title="4.3 客户应用"></a>4.3 客户应用</h4><ol><li><p>客户应用会首先向主服务器发送元信息请求</p></li><li><p>在拿到元信息后，客户应用会向所有分块服务器中的某一个发送数据块请求。通常是选择“最近”的一个分块服务器，这里的“最近”是在IP层的意义下的最近</p></li><li><p>客户应用侧不缓存文件数据，只缓存文件元信息。从使用场景假设可以看出顺序读取和append操作并不会得益于客户应用侧的缓存机制，并且除去缓存以后也不用担心缓存一致性的问题</p></li><li><p>不支持所有的POSIX文件系统接口，支持大多数文件操作，包括创建、删除、打开、关闭、读取和写入</p></li><li><p>支持两个特殊的操作</p><ol><li>append，这个操作的好处是不需要进行显示的并发同步即可保证并发的append操作不会相互影响</li><li>snapshot，这个操作能够很高效地复制一个文件或一个目录结构（即目录下的所有文件）</li></ol><p>关于这两个操作的细节将在后问阐述。</p></li></ol><h3 id="5-GFS的读与写操作"><a href="#5-GFS的读与写操作" class="headerlink" title="5. GFS的读与写操作"></a>5. GFS的读与写操作</h3><h4 id="5-1-GFS的读操作流程"><a href="#5-1-GFS的读操作流程" class="headerlink" title="5.1 GFS的读操作流程"></a>5.1 GFS的读操作流程</h4><p><img src="/images/image-20200628211850013.png" alt="GFS读操作的流程"></p><p>读操作相对比较直观，将之前的框架图稍加标记即可得到一个完整的读操作的执行流程</p><ol start="0"><li><p>客户应用运用已知的固定分块大小，即64MB，以及客户所请求的文件偏移量，即可算出该偏移量所在分块的编号</p></li><li><p>客户应用将这个分块编号以及文件名发送到主服务器，请求对应的文件元信息</p></li><li><p>主服务器将对应的文件元信息返回给客户应用，其中包括文件块的句柄以及实际存储的分块服务器信息</p><p>客户应用会将该元信息缓存到本地，缓存所用的键是文件名和分块编号</p></li><li><p>客户应用向“最近”的分块服务器发送数据请求，指明对应的块句柄和数据范围</p></li><li><p>分块服务器返回相应的文件块</p></li></ol><p>注：</p><ul><li><p>在实际实现中，主服务器会多回复一些紧跟在请求文件块之后的文件块元信息，类似于prefetch的思想</p></li><li><p>云信息的缓存拥有给定的生存时间（Time To Live）；亦或者文件在客户应用侧被关闭后再次打开时，元信息缓存也会失效</p></li></ul><h4 id="5-2-GFS的写操作流程"><a href="#5-2-GFS的写操作流程" class="headerlink" title="5.2 GFS的写操作流程"></a>5.2 GFS的写操作流程</h4><p>前面提到GFS默认会为每一个文件块保持三个备份，那么当这个文件块需要被修改的时候，如何保持这些备份同步得到更新就是需要解决的一个重要问题。</p><h5 id="5-2-1-主要分块服务器"><a href="#5-2-1-主要分块服务器" class="headerlink" title="5.2.1 主要分块服务器"></a>5.2.1 主要分块服务器</h5><p>前面我们提到，GFS默认为每一个文件块提供三个分块服务器予以储存。这三个分块服务器有没有层级关系呢？答案是肯定的。其中一个分块服务器将会作为该文件块的主要分块服务器（primary chunk server），其他的为次要分块服务器（secondary chunk server）。这个主要分块服务器的作用就体现在写操作上，在具体到写操作之前，我们先来看看主要分块服务器是如何确立的。</p><p>在GFS中，（每一个文件块的）主要分块服务器的身份是由主服务器决定的，并且通过lease来管理这个身份。一个lease通常为60s，即这个时间可以大致理解为各个分块服务器向主服务器汇报自身状态的时间间隔。之所以通过lease来管理，就是为了应对这样一种情况——某个主要分块服务器挂了，那么在lease到期后，主服务器没有收到该主要分块服务器的汇报，那么此时主服务器就可以默认该分块服务器已经不能正常工作了，接下来就会重新从次要分块服务器中选择一个来作为新任的主要分块服务器。</p><h5 id="5-2-2-控制流与数据流"><a href="#5-2-2-控制流与数据流" class="headerlink" title="5.2.2 控制流与数据流"></a>5.2.2 控制流与数据流</h5><p>GFS中涉及写操作的流程大致可以分为两类，控制流与数据流（如下图所示）。</p><p><img src="/images/image-20200701172343492.png" alt="GFS写操作的流程"></p><ul><li><p>数据流</p><p>我们可以看到这里的数据流是一个串型流，而不是类似于广播式的并行流。这样做的考量是由于某一台机器的带宽是有限的，如果这里让<code>Client</code>端发送三分数据到三个分块服务器的话，对<code>Client</code>端带宽的占用会比较大。而使用图中这种串型传递的方式能够有效的分散带宽压力。</p><ul><li>从这一点，我们也可以发现整个GFS就是为大文件考虑的，在这种模式下，如果是数据量较小，那么会导致网络延迟增加。但如果数据量较大，那么网络延迟的增加就会很小，甚至还会降低网络延迟（如果接受不了到数据的一端把数据拆分成更小的粒度直接传递下去的话）。</li><li>当数据到达分块服务器后，写操作不会马上进行，而是由分块服务器暂时缓存该数据，等待后续的控制流。</li></ul></li><li><p>控制流</p><p>图中的第一、二步还是和之前相同，即从Master获取元信息。第四步就是客户应用向主要分块服务器发送写操作请求，而剩下的控制流都由该主要分块服务器来引导。其中包括第五步，由主要分块服务器向所有的次要分块服务器发送写操作请求，如果成功，各个次要分块服务器会回复主要分块服务器（第六步），等所有次要分块服务器都回复后，并且自身也完成写操作后，该主要分块服务器就会向客户应用返回成功信息（第七步）。</p><ul><li>如果存在多个针对该文件块的并发写操作，由于这些写操作请求都会经过主要分块服务器，因此在主要分块服务器这边会完成顺序的统一，即它会给每个写操作请求编号，要求所有次要分块服务器按照一致的顺序来执行这些并发写操作。</li><li>这里也可以看出append操作的原子性，即客户应用在发起append操作时并不会直接指定append的起始偏移量（相对于文件开始的位置），这一决定完全由主要分块服务器来做，即主要分块服务器会分配文件偏移量。这样就可以使得并发append操作有良好的原子性，并且不需要借助于锁的概念。</li><li>简单的情况是当append操作的数据量能够装进该文件块。如果不能的话，GFS的选择是对当前文件块进行填充，然后告诉客户应用append到下一个文件块。</li><li>并且值得注意的是，在客户应用侧，GFS采取的语义是至少执行一次。这就表明了，如果一次写操作由主要服务器认定为不成功，那么客户应用会再次请求该写操作，直到成功为止。而这种不成功，可能是某一个次要分块服务器的不成功，也可能是主要分块服务器的不成功。因此，例如一次append操作可能带来的结果是同样的数据被append了多于1次，也可能各个分块服务器的append次数不同等等。这个时候，备份之间的一致性就被打破了。后面会讨论GFS如何应对。</li></ul></li></ul><h3 id="6-GFS的容错机制"><a href="#6-GFS的容错机制" class="headerlink" title="6. GFS的容错机制"></a>6. GFS的容错机制</h3><ul><li>分块服务器一般有三个备份</li><li>主服务器也会有多个备份</li><li>一个分块服务器如果崩了，在下一次主服务器进行全局扫描任务的时候，会重新起一个分块服务器</li><li>每次主服务器重新给主要分块服务器lease的时候会附加一个版本号，这个版本号也会传给同一个文件块的其他分块服务器</li><li>主服务器通过检测版本号来检测是否某个分块服务器已经过时了，即崩溃过、重启过等等，此时该分块服务器就会被主服务器的垃圾回收机制给回收掉。即某个分块服务器一旦出现任何故障，哪怕最后复原了，也会被作为垃圾回收</li></ul><h3 id="一些重要的设计及问题"><a href="#一些重要的设计及问题" class="headerlink" title="一些重要的设计及问题"></a>一些重要的设计及问题</h3><ol><li><p>主服务将所有的元信息都存储在内存（RAM）中，而非硬盘。这样做使得主服务对元信息的读取效率非常快。而这样做的一个潜在隐患即主服务器的内存空间有限，这样做真的好吗？</p><p>由于一个文件对应的元信息相对于文件大小来说是很小的。GFS所选用的分块大小是64MB，而通常一个64MB的文件分块所对应的元信息只有不到64B。并且根据GFS的使用场景假设，大文件居多。因此，这样的元信息所带来的存储overhead是很小的，百万分之一。论文表明实践中内存容量并没有成为限制整个文件系统大小的一个因素。再退一步讲，增加内存容量所需要的成本与将元信息存储在内存中所带来的好处相比是微乎其微的。</p><p>将元信息存储在内存中所带来的读取速度的提升，使得主服务器可以在后台周期性地进行全文件扫描。而这样的扫描是主服务器能够实现垃圾回收、数据块迁移、分块服务器复制等任务的重要前提。</p></li><li><p>分块大小为64MB，为什么选取这样一个较大的文件大小？</p></li><li><p>关于主要分块服务器的身份管理，不使用lease可以吗？</p></li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li>CMU <a href="http://www.andrew.cmu.edu/course/15-440/" target="_blank" rel="noopener">15-640</a></li><li><a href="https://dl.acm.org/doi/pdf/10.1145/945445.945450" target="_blank" rel="noopener">原论文</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式系统 </tag>
            
            <tag> 文件系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习笔记 - Java多线程——synchronized关键字</title>
      <link href="/2020/06/30/2020-06-30-java-synchronized/"/>
      <url>/2020/06/30/2020-06-30-java-synchronized/</url>
      
        <content type="html"><![CDATA[<p><code>synchronized</code>作为Java的一个关键字，是实现Java多线程编程中互斥访问的重要手段之一。简单的来说，synchronized所修饰的部分一定有一个明确的对象，所有被synchronized修饰同一对象的代码块，在同一时间至多只有一个会被某一个线程访问。</p><p>我们知道在C中实现多线程互斥访问的主要方法是互斥锁，而互斥锁又是一种特殊的信号量。那么Java中的synchronized在底层是如何实现的？被synchronized修饰的静态方法、实例方法（instance method）、静态方法中的代码块、实例方法中的代码块有什么区别？</p><h3 id="1-synchronized语义"><a href="#1-synchronized语义" class="headerlink" title="1. synchronized语义"></a>1. synchronized语义</h3><h4 id="1-1-被synchronized修饰的实例方法"><a href="#1-1-被synchronized修饰的实例方法" class="headerlink" title="1.1 被synchronized修饰的实例方法"></a>1.1 被synchronized修饰的实例方法</h4><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">synchronized</span> <span class="token keyword">void</span> <span class="token function">methodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// do something</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">synchronized</span> <span class="token keyword">void</span> <span class="token function">methodB</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// do something</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这种情况下的语义表明一个实例中所有被synchronized修饰的实例方法，在同一时间至多只有一个线程进入这些方法其中的一个。而<strong>不同实例则互不影响</strong>。即同一时间，该实例的方法A和方法B至多只会有一个被某一个线程调用。</p><p>在语义上，以下两种等同。</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">// 1</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">synchronized</span> <span class="token keyword">void</span> <span class="token function">methodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// do something</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 2</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">methodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">synchronized</span> <span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// do something</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h4 id="1-2-被synchronized修饰的静态方法"><a href="#1-2-被synchronized修饰的静态方法" class="headerlink" title="1.2 被synchronized修饰的静态方法"></a>1.2 被synchronized修饰的静态方法</h4><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">synchronized</span> <span class="token keyword">void</span> <span class="token function">staticMethodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// do something</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>由于静态方法是属于整个类的，因此，在这个情况下，synchronized实际修饰的对象是整个类。并且在Java虚拟机中，一个类只会有一个实体。因此，对于这个类以及所有这个类的实例来说，同一时间至多只有一个线程能够进入被synchronized修饰的静态方法。<strong>不同的类则互不影响</strong>。</p><p>在语义上，以下两种等同。</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">// 1</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">synchronized</span> <span class="token keyword">void</span> <span class="token function">methodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment" spellcheck="true">// do something</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// 2</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">methodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">synchronized</span> <span class="token punctuation">(</span>Test<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// do something</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h4 id="1-3-被synchronized修饰的实例方法中的代码块"><a href="#1-3-被synchronized修饰的实例方法中的代码块" class="headerlink" title="1.3 被synchronized修饰的实例方法中的代码块"></a>1.3 被synchronized修饰的实例方法中的代码块</h4><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> <span class="token keyword">int</span> count<span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">methodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">synchronized</span> <span class="token punctuation">(</span>count<span class="token punctuation">)</span> <span class="token punctuation">{</span>              <span class="token comment" spellcheck="true">// do something</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p><code>synchronized</code>关键字修饰代码块时需要显示给出其所修饰（同步）的对象，这里的对象一般为多线程中需要同步的共享变量、资源。这个对象也被称为<code>monitor object</code>（监视器对象）。在一个进程中，至多只有一个线程可以进入被同一个监视器对象修饰的代码块。</p><h4 id="1-4-被synchronized修饰的静态方法中的代码块"><a href="#1-4-被synchronized修饰的静态方法中的代码块" class="headerlink" title="1.4 被synchronized修饰的静态方法中的代码块"></a>1.4 被synchronized修饰的静态方法中的代码块</h4><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Test</span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">int</span> count<span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">methodA</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">synchronized</span> <span class="token punctuation">(</span>count<span class="token punctuation">)</span> <span class="token punctuation">{</span>              <span class="token comment" spellcheck="true">// do something</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这里没什么特别的，与实例方法差不多，不同点就是静态方法所能够访问的变量都必须是静态的。</p><h3 id="2-synchronized的局限和一些替代品"><a href="#2-synchronized的局限和一些替代品" class="headerlink" title="2. synchronized的局限和一些替代品"></a>2. synchronized的局限和一些替代品</h3><ol><li>synchronized是严格互斥的，机不允许多于一个线程同时进入被同一个监视器对象修饰的区域。但是有些时候，如果一些线程是只读的话，是可以允许这样的多个线程同时进入的（为了提高效率）。这种情况一般会选择使用读写锁机制（Reader/writer lock），Java也提供了这样一个类<code>ReentrantReadWriteLock</code>。</li><li>同时synchronized也不能直接用于对共享资源的数量进行同步的工作，即通常所说的生产者消费者机制。这种情况的话可以通过信号量来解决，Java也提供了信号量的类<code>Semaphore</code>。</li></ol><h3 id="3-synchronized是可重入的"><a href="#3-synchronized是可重入的" class="headerlink" title="3. synchronized是可重入的"></a>3. synchronized是可重入的</h3><p>考虑下面的这个例子，你觉得终端会输出什么？会报错吗？</p><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">PlayClass</span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> <span class="token keyword">int</span> count<span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">add</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">synchronized</span> <span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">++</span>count <span class="token operator">&lt;</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token function">add</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>count<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        PlayClass p <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">PlayClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        p<span class="token punctuation">.</span><span class="token function">add</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>答案是会输出5个5！这就表明synchronized的运行机制与我们熟知的C中的互斥锁是非常不一样的，前面也提到了监视器对象（monitor object），那么这个监视器是什么呢？</p><h3 id="4-synchronized的实现机制——Monitor"><a href="#4-synchronized的实现机制——Monitor" class="headerlink" title="4. synchronized的实现机制——Monitor"></a>4. synchronized的实现机制——Monitor</h3><h3 id="4-1-Monitor"><a href="#4-1-Monitor" class="headerlink" title="4.1 Monitor"></a>4.1 Monitor</h3><p>在JVM中，每一个对象（object）和类（class）都有与之对应的监视器（原生类型没有监视器），监视器有与之对应的互斥锁，线程通过获取该互斥锁来占有监视器所保护的资源。对于对象来说，监视器所保护的资源是对象的实例变量（instance variables）；对类来说，监视器所保护的资源是类变量（class variables）。</p><p>每一个监视器都有一个线程等待集合（wait set），这个集合在对象初始化或者类加载的时候是空的。这个集合的后续操作只能通过<code>Object</code>类的三个实例方法来实现，<code>wait()</code>，<code>notify()</code>，<code>notifyAll()</code>。</p><p>其中<code>wait</code>方法是一个占有资源的线程主动释放资源的接口，调用该方法意味着该线程释放了监视器的锁并进入了线程等待集合。<code>notify</code>和<code>notifyAll</code>都是当前占有资源的线程给正在线程等待集合中的线程发送唤醒信号的接口，<code>notify</code>是给某一个等待集合中的线程发（取决于线程调度，如果有多个的话），<code>notifyAll</code>是给所有等待集合中的线程发。不过，这并没有任何实际意义，即并不是说被唤醒的线程就立即获得了监视器的锁，因为当前线程并没有释放锁。所以这个信号仅仅只是一个信号，被唤醒的线程还需要通过调度获得监视器的锁才算真正占有了资源。如果没有其他未进入等待集合的线程，那么被唤醒的线程会在当前线程释放锁之后获得监视器的锁。如果有其他未进入等待集合的线程，那么有可能新增的线程获得了锁，而之前被唤醒的等待线程只能再次进入等待集合。</p><p>整个监视器的逻辑结构大致如下所示</p><p><img src="/images/image-20200630185317040.png" alt="Java Monitor的逻辑结构"></p><p>中间的部分即监视器所保护（同步）的资源，右侧是线程等待集合，左侧是未进入等待集合的线程（可以理解为第一次尝试进入监视器所保护区域的线程）。从图中可以看出，线程必须获得锁才能占有监视器所保护的资源。当前占有资源的线程可以通过<code>wait</code>进入等待集合，或是运行结束后退出。由此，我们也可以理解为什么前面提到的<code>synchronized</code>是可重入的，即当前占有资源的线程，只要不选择等待或者退出，那么它就一直占有资源，它不需要再次获取锁。</p><p>以下有几个值得注意的地方</p><ul><li>由于<code>A notify B</code>和<code>B 占有资源</code>两者之间并没有必然联系，并且A在发送信号给B之后也可以继续占有资源并做一些操作。因此对于B来说，一个良好的习惯是每次占有资源后，需要对自己所关心的<strong>条件</strong>重新检验，如果不符合，则再次调用<code>wait</code>进入等待集合。</li><li>如果当前线程退出前，所有等待集合中的线程均没有被唤醒（被当前线程唤醒，或者是<code>wait</code>设置了超时），那么只有处于<code>entry set</code>的线程才会竞争监视器的锁。</li><li><code>notify</code>操作究竟会唤醒哪一个等待集合中的线程是随着底层实现而变的，可能是FIFO，可能是LIFO，也可能是其他的调度算法。</li><li>对于类（class）来说，监视器实际保护的对象是该类所对应的类实例，<code>java.lang.Class</code>对象。</li><li>监视器的锁实际上是一个计数器（相对于二元互斥锁），当前占有监视器资源的线程可以再次获得该锁（也就是我们前面的那个例子），每次获得该锁，这个计数器就会增加1，而每次释放这个锁，计数器就会减去1，直到计数器为0，表明该线程完全释放了锁，也即线程释放了监视器所保护的资源。</li></ul><h4 id="4-2-Nested-Monitor-Lockout"><a href="#4-2-Nested-Monitor-Lockout" class="headerlink" title="4.2 Nested Monitor Lockout"></a>4.2 Nested Monitor Lockout</h4><p>在出现synchronized嵌套的时候，不良的编程习惯同样会导致死锁。而monitor的机制本身可能带来另一个问题——Nested Monitor Lockout（嵌套管程锁死）。</p><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">Lock</span><span class="token punctuation">{</span>    <span class="token keyword">protected</span> MonitorObject monitorObject <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">MonitorObject</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">protected</span> <span class="token keyword">boolean</span> isLocked <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">lock</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> InterruptedException<span class="token punctuation">{</span>        <span class="token keyword">synchronized</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">while</span><span class="token punctuation">(</span>isLocked<span class="token punctuation">)</span><span class="token punctuation">{</span>                <span class="token keyword">synchronized</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">.</span>monitorObject<span class="token punctuation">)</span><span class="token punctuation">{</span>                    <span class="token keyword">this</span><span class="token punctuation">.</span>monitorObject<span class="token punctuation">.</span><span class="token function">wait</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            isLocked <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">unlock</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">synchronized</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">this</span><span class="token punctuation">.</span>isLocked <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span>            <span class="token keyword">synchronized</span><span class="token punctuation">(</span><span class="token keyword">this</span><span class="token punctuation">.</span>monitorObject<span class="token punctuation">)</span><span class="token punctuation">{</span>                  <span class="token keyword">this</span><span class="token punctuation">.</span>monitorObject<span class="token punctuation">.</span><span class="token function">notify</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>这个问题出现的情形大致如下</p><ol><li>线程A占有了监视器对象<code>this</code>，此时<code>isLocked = true</code>，线程A占有<code>this.monitorObject</code>之后主动进入线程等待集合，等待另一个线程唤醒自己</li><li>线程B想要唤醒A，但是线程B需要首先占有监视器对象<code>this</code></li></ol><p>这里就发生了嵌套管程锁死问题，其根本的原因就是因为线程A在等待B的唤醒，但与此同时线程A仍然占有监视器对象<code>this</code>，而线程B唤醒A的前提是先要占有监视器对象<code>this</code>，因此造成了锁死。需要注意的是这里的锁死和通常意义的死锁有略微的不同。通常意义的死锁是指双方占有不同的资源而又需要占有对方已经占有的资源而导致的锁死问题。从这一点上来说嵌套管程锁死并不是死锁，但是我个人认为从结果上来看称为死锁似乎也合理。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="http://tutorials.jenkov.com/java-concurrency/synchronized.html" target="_blank" rel="noopener">Java Synchronized Block</a></li><li><a href="https://www.programcreek.com/2011/12/monitors-java-synchronization-mechanism/" target="_blank" rel="noopener">Monitors - The basic idea of Java synchronization</a></li><li><a href="https://medium.com/swlh/difference-between-java-monitor-and-lock-4677c1b6715f" target="_blank" rel="noopener">Difference between Java monitor and lock</a></li><li><a href="https://www.artima.com/insidejvm/ed2/threadsynchP.html" target="_blank" rel="noopener">Thread Synchronization</a></li><li><a href="http://tutorials.jenkov.com/java-concurrency/nested-monitor-lockout.html" target="_blank" rel="noopener">Nested Monitor Lockout</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> 多线程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习笔记 - HTTP，cookie和session</title>
      <link href="/2020/06/29/2020-06-29-http-cookie-session/"/>
      <url>/2020/06/29/2020-06-29-http-cookie-session/</url>
      
        <content type="html"><![CDATA[<h3 id="1-从HTTP-0-9-到-HTTP-1-1"><a href="#1-从HTTP-0-9-到-HTTP-1-1" class="headerlink" title="1. 从HTTP/0.9 到 HTTP/1.1"></a>1. 从HTTP/0.9 到 HTTP/1.1</h3><p>HTTP协议作为一个应用层协议，应该是出现频率最高的一个协议了。几乎所有的网站都是HTTP协议。从最初的HTTP，再到后来的HTTPS，其内部的版本主要经历了0.9，1.0，1.1以及未来将会普及的2.0。关于HTTP协议更详细的内容具体可参见每个版本对应的RFC，例如当前普及率最高的HTTP/1.1版本的RFC编号是2616。所谓RFC，即Request For Comments，是一种类似于业内共识的文本。</p><h4 id="1-1-HTTP-0-9"><a href="#1-1-HTTP-0-9" class="headerlink" title="1.1 HTTP/0.9"></a>1.1 HTTP/0.9</h4><ul><li>仅支持方法：<code>GET</code></li><li>仅支持超文本类型</li><li>连接在得到回复后立即结束</li><li>不支持HTTP header</li><li>没有状态码（即400，408这些）</li></ul><h4 id="1-2-HTTP-1-0"><a href="#1-2-HTTP-1-0" class="headerlink" title="1.2 HTTP/1.0"></a>1.2 HTTP/1.0</h4><ul><li>支持方法：<code>GET</code>, <code>HEAD</code>, <code>POST</code></li><li><strong>支持更多类型文件</strong>，包括脚本和媒体文件等</li><li>连接在得到回复后立即结束</li><li><strong>支持HTTP header</strong>，包含一些请求/回复的信息，例如文本类型、上次修改时间等</li><li><strong>支持状态码</strong></li></ul><p>至此，可以看到从0.9到1.0，有一个共同的问题没有得到解决，即每一次连接都会在得到回复后立即结束。这里我们默认双方使用的是TCP作为数据传输层协议，那么每一次连接都需要进行建立连接的三次握手阶段，这一阶段需要至少3倍的单向延迟时间（1.5个RTT），很明显当一个客户需要向服务器请求多个实体时这样频繁的建立连接是没有必要且十分耗时的。HTTP/1.1版本可以说主要是解决了这个问题。</p><h4 id="1-3-HTTP-1-1"><a href="#1-3-HTTP-1-1" class="headerlink" title="1.3 HTTP/1.1"></a>1.3 HTTP/1.1</h4><ul><li>支持方法：<code>GET</code>, <code>HEAD</code>, <code>POST</code>, <code>PUT</code>, <code>DELETE</code>, <code>TRACE</code>, <code>OPTIONS</code></li><li><strong>支持持久化的连接（keep-alive）</strong></li><li><strong>支持串联请求（pipelined requests）</strong></li><li><strong>支持并发处理请求（I/O多路复用）</strong></li></ul><h3 id="2-从HTTP-1-1-到-HTTP-2-0"><a href="#2-从HTTP-1-1-到-HTTP-2-0" class="headerlink" title="2. 从HTTP/1.1 到 HTTP/2.0"></a>2. 从HTTP/1.1 到 HTTP/2.0</h3><p>虽然HTTP/1.1版本是目前流行的版本，但是它本身也有一些问题待解决。其中一个最主要的问题就是队头阻塞问题（Head-of-line Blocking）。这个问题指的是在持久化连接、尤其是串联请求的时候，如果前面有一个请求话费的时间较长（例如需要传输的文件较大），那么后面所有的请求都需要等待该请求完成后才能进行。</p><p>一个简单的例子，假设你在浏览一篇博客，但是这篇博客里面有一张很大的图片，并且你的带宽并不是很高的情况下，我们假设传输这张图片需要2秒。而恰巧，你的浏览器对这篇博客的HTTP串联请求，将这个图片放在了一开始的地方，那么此时，你的浏览器将会处于空白状态。2秒之后这张图片显示出来了，随即这篇博客的正文也显示出来了。但是其实这张图片并不是你关注的重点，你只是想看博客的文字内容。所以在这种情况下，之前等待的2秒是非常影响用户体验的。</p><p>HTTP/2.0 版本便主要针对这个问题，提出了一些解决方案。</p><ul><li>TCP连接多路复用<ul><li>支持在同一个TCP连接内，采用多路复用（多个流，multiple streams）的方式，并发地传输串联请求所需要的文件，因此小文件不会被大文件阻塞</li><li>主要是通过将各个文件切分成小粒度的message，然后进行编号，让客户端能够进行文件重组</li></ul></li><li>流优先级管理（Stream Prioritization）<ul><li>这个是建立在多路复用的基础上的，可以使得服务端能够进一步定制页面的加载顺序，以满足定制化的用户需求（例如有的场景需要优先加载图片，有的场景则需要优先加载文本，等等）</li><li></li></ul></li><li>服务器推送（server push）<ul><li>在这之前，所有的HTTP请求都是以request for A –&gt; response with A的形式存在的，即客户端要什么，服务端就给什么。但是实际应用场景，每个网页都是由很多文件内容组成的，并且会有一些依赖关系（例如一个HTML文件里面有一个图片的link，那么大概率客户端会请求那个图片）</li><li>服务器端便可以通过这种依赖关系来向客户端主动发送一些文件内容</li></ul></li><li>请求头压缩<ul><li>HTTP/1.1其实也有对message进行压缩，是对message body，即对实际传输的文件（CSS、js脚本等）进行压缩</li><li>HTTP/2.0引入的是对请求头信息的压缩，采用的是（HPACK）</li></ul></li></ul><h3 id="3-HTTP的扩展"><a href="#3-HTTP的扩展" class="headerlink" title="3. HTTP的扩展"></a>3. HTTP的扩展</h3><p>HTTP最初在设计上是去状态化（stateless）的，即HTTP服务器是不需要存储用户的相关状态信息的，去状态化最大的优势即能够大大降低服务器的载荷，一个连接在结束之后，与其相关的所有资源均可释放，使得服务器端的逻辑更简洁，这很符合HTTP协议的定位。但是自从1.1版本开始，随着持久化连接、串联请求以及并发请求的引入，更复杂的网站被建立起来，其中最主要的一个特征就是有着用户账号体系，需要登录的网站服务。因此，这些网站服务就需要跨请求传递信息的支持，例如网站服务需要辨认多个请求是否来自同一个账号（身份认证），或者网站服务需要对用户行为进行跟踪和分析，又或者网站服务需要为用户定制化内容等。Cookie和session的引入正是为了满足这些需求。</p><h4 id="3-1-Cookie"><a href="#3-1-Cookie" class="headerlink" title="3.1 Cookie"></a>3.1 Cookie</h4><p>Cookie可以理解为是HTTP服务器主动在用户侧存储的一些非敏感信息，以键值对的形式储存。</p><ul><li>每一条cookie记录有着对应的作用域，当一个请求满足某条cookie的作用域时，浏览器就会自动将该cookie连同请求一起发送（作为请求头的一部分）。</li><li>每一条cookie记录有对应的生存周期，如果指定生存周期，那么浏览器就会将cookie储存起来（硬盘），否则该cookie会在当前会话结束（浏览器窗口关闭）后被自动删除。</li><li>不同的浏览器不会共享cookie，即便是同一个作用域下的cookie。</li><li>一个网站服务只能读取属于它的作用域内的cookie，这一点是由浏览器来保证的。</li></ul><p>前面提到cookie的一个主要应用就是进行身份认证，也就通常的用户登录后的界面。而我们知道cookie是储存在客户端的，并且跨作用域访问限制是完全依靠浏览器实现的，因此，可以看出用cookie直接储存敏感信息是不可取的。那么既然如此，cookie又怎么能够实现身份认证这种需要敏感信息参与的任务呢？这就引出了session机制。可以说，session机制是建立在cookie机制之上的，并且session是需要在服务端存储相应信息的，这一点与cookie不同。</p><h4 id="3-2-Session"><a href="#3-2-Session" class="headerlink" title="3.2 Session"></a>3.2 Session</h4><p>这里用一个具体的例子来阐述session是如何建立在cookie的基础上运作的。</p><ol><li>用户在浏览器上登录Facebook账号，采用的是POST方法，里面包含了用户名和密码（没有cookie参与）</li><li>Facebook服务器收到该请求后会匹配用户数据库，如果匹配成功则用户成功登录。服务器会记录下该用户的登录状态，并分配一个唯一（对Facebook服务器端唯一）的编号（称为session id）给该用户账号</li><li>在回复中服务器会加入<code>Set-Cookie</code>头让用户端的浏览器将该编号作为cookie内容存储下来，通常会设置生存周期和服务器端的自动登出时间一致</li><li>这之后，如果用户访问Facebook的相关页面，浏览器就会自动将刚才包含session编号的cookie附加在请求头中</li><li>Facebook再次收到该用户的请求后，会识别该session id，并与自身维护的已登录用户做匹配</li><li>如果匹配成功，那么Facebook服务器就完成了对用户身份的认证，随即可以为该用户输出定制化的页面，例如好友列表、好友动态等</li></ol><p>注意：这里的身份认证其实有一些歧义，因为整个过程中，用户完成登录是不需要cookie或者session的介入的。而后续服务器端仅仅是识别该已登录用户的请求，所以称为身份识别可能会更精确一些。</p><p>不过从这一点也可以窥探出cookie或者session机制会带来的问题——即如果我获得了别人的cookie，是否意味着我可以假装别人获取已登录界面？一般来说服务端还会检查用户IP地址等，但是诸如此类也可以被伪装或者绕过。因此这个问题的回答倾向于是肯定的。所以，绝大多数网站都仅仅会依据session的匹配结果给出一些非敏感的个性化信息，例如商品推荐等。但是一旦涉及到类似下单、结算等比较敏感的操作，服务网站会采取更严格的措施，例如重新登录等，而不仅仅依靠cookie或者session信息。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://medium.com/platform-engineer/evolution-of-http-69cfe6531ba0" target="_blank" rel="noopener">Evolution of HTTP</a></li><li><a href="https://www.jianshu.com/p/831bd379d1b6" target="_blank" rel="noopener">session-理解HTTP</a></li><li><a href="https://stackoverflow.com/questions/3804209/what-are-sessions-how-do-they-work" target="_blank" rel="noopener">what are sessions how do they work</a></li><li><a href="https://blog.webf.zone/ultimate-guide-to-http-cookies-2aa3e083dbae" target="_blank" rel="noopener">Ultimate guide to http cookies</a></li><li><a href="https://www.cloudflare.com/learning/performance/http2-vs-http1.1/" target="_blank" rel="noopener">Why is HTTP/2.0 faster than HTTP/1.1?</a></li><li><a href="https://www.digitalocean.com/community/tutorials/http-1-1-vs-http-2-what-s-the-difference" target="_blank" rel="noopener">HTTP/1.1 v.s. HTTP/2.0: What’s the difference?</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机网络 </tag>
            
            <tag> HTTP </tag>
            
            <tag> Cookie </tag>
            
            <tag> Session </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习笔记 - 自底向上理解 B树/B+树</title>
      <link href="/2020/06/27/2020-06-27-b-tree/"/>
      <url>/2020/06/27/2020-06-27-b-tree/</url>
      
        <content type="html"><![CDATA[<p>本文可以理解为是<a href="https://www.youtube.com/watch?v=aZjYr87r1b8" target="_blank" rel="noopener">这个视频</a>的笔记，感兴趣的同学强烈推荐观看原视频，作者阐述深入浅出，十分完善。</p><p>Abdul在这个视频开始就说了一句非常精彩的话</p><blockquote><p>在内存里管理数据用的是数据结构，而在硬盘里管理数据用的都属于数据库管理系统（DBMS）</p></blockquote><p>所以B树/B+树和一般的数据结构相比，例如红黑树或者二叉树等，给人的感觉上很不一样。这种不一样最明显的体现就是，B树的来源，即B树是在怎么样的背景下被引导出来的？为什么B树有着这样的性质和这样的插入方式？这些问题都会在本文中得到解答。</p><h3 id="1-机械硬盘的基本性质"><a href="#1-机械硬盘的基本性质" class="headerlink" title="1. 机械硬盘的基本性质"></a>1. 机械硬盘的基本性质</h3><ul><li>机械硬盘（spinning disk），可以简单地理解为一个圆盘被划分成了很多同心圆环，同时又被划分为很多扇形区域。圆环称为轨道（track），而扇形区域称为片区（sector）</li><li>每一个轨道和片区的交集称为一个块（block）</li><li>每一个块有着相同的大小，例如 512B</li><li>所以根据以上的性质，想要访问硬盘上的某一个字节（Byte），需要轨道编号、片区编号、以及block内的偏移量（offset），这样就可以完全对应上一个字节。</li></ul><p><img src="/images/image-20200627150112583.png" alt="机械硬盘简图"></p><h3 id="2-数据索引"><a href="#2-数据索引" class="headerlink" title="2. 数据索引"></a>2. 数据索引</h3><p>假设我们现在有一组数据，里面有一百条记录（可以理解为键值对），每一条记录占用的空间是128字节。那么很容易就可以得到，一个512字节大小的block能够存储4条记录，为了存储100条记录，最少需要25个block。此时如果我们需要按键查找某条记录，那么我们最多需要访问25个block，才能找到相对应的记录。本文的剩余内容就是围绕如何降低最多需要遍历的block数量展开的。</p><p>由此便有了数据索引的概念，即将每一条记录的键和该条记录对应的硬盘地址单独拿出来组成一条新的记录。假设我们需要10个字节来存储一个键，6个字节来存储一个硬盘地址。因此，需要16个字节来存储一条数据索引，总共需要1600字节来存储整个100条记录的索引。意味着这个索引需要至少4个block来存储。到这里，我们的按键查找操作所需要访问的最大block数目已经从原先的25个降低到了5个（4个索引block，1个存记录的block）。</p><h3 id="3-多层数据索引"><a href="#3-多层数据索引" class="headerlink" title="3. 多层数据索引"></a>3. 多层数据索引</h3><p>可以看到数据索引对按键查找的优化是很显著的，那么还可以更进一步吗？由此便引出了多层索引，即索引的索引。之前的索引可以认为只是将键和地址取出，以压缩其他的信息。那么多层索引则是运用了类似二叉查找的思想，将所有一级索引（将之前的索引简称为一级索引）按键排序并分块。不妨假设每一块包含32个一级索引，那么之前的100个一级索引需要至少4个二级索引。4个索引只需占用一个block，至此按键查找所需访问的最大block数目从5个又降低到了3个（一个二级索引，一个一级索引，一个数据记录）。整个结构大致如下图所示。</p><p><img src="/images/image-20200627155043118.png" alt="多层索引结构"></p><h3 id="4-多路查找树"><a href="#4-多路查找树" class="headerlink" title="4. 多路查找树"></a>4. 多路查找树</h3><p>如果把上图的结构旋转一下，就可以得到类似B树的结构。在正式引出B树之前，还需要有一个概念，那就是多路查找树，或者称为m路查找树，一般的二叉搜索树即是二路查找树。<code>m</code>称为多路查找树的阶数。该树有如下性质</p><ul><li>m路查找树的任意一个非叶子结点至多拥有<code>m</code>个孩子节点。</li><li>每一个节点至多有<code>m-1</code>个键</li></ul><h3 id="5-B树-多路查找树-额外性质"><a href="#5-B树-多路查找树-额外性质" class="headerlink" title="5. B树 = 多路查找树 + 额外性质"></a>5. B树 = 多路查找树 + 额外性质</h3><p>这些额外性质包括</p><ul><li>任意一个非叶子结点至少包含<code>Math.ceil(m / 2)</code>个孩子节点， 根节点例外（根结点至少有两个孩子节点）</li><li>所有叶子结点均在同一层，即所处树的深度一致</li><li>插入操作是自底向上进行的</li></ul><p>其中最后一点是B树的精华所在，即索引的结构及数量是根据实际数据量自动调整的，这也符合我们前面所提到的索引构建过程，即数据–&gt;一级索引–&gt;二级索引–&gt;……</p><p>具体的插入过程网上已经有大量的文章了，这里就不赘述了。同时，删除操作其实和插入操作的逆向很相似。并且正如Abdul说的那样，大家不用太在意在插入删除过程中的一些细节问题，例如结点分裂的时候到底是位于<code>Math.ceil(m / 2)</code>的键上移还是位于<code>Math.ceil(m / 2) + 1</code>的键上移，结点合并的时候到底是和左兄弟结点合并还是右兄弟。这些细节问题完全取决于具体实现者的偏好，只需记住以上的性质不被破坏即可。</p><h3 id="6-B-树-B树-额外性质"><a href="#6-B-树-B树-额外性质" class="headerlink" title="6. B+树 = B树 + 额外性质"></a>6. B+树 = B树 + 额外性质</h3><p>这里的额外性质主要就是一条</p><ul><li>所有的非叶子结点均不存储实际的索引，只存键</li></ul><p>这样做的目的或者说好处即在非叶子结点可以存储的键更多了，因为非叶子结点不再需要存储实际的指向硬盘数据的指针，因此可以存储更多的键以及更多的指向孩子节点的指针（即分叉更多）。所以同样结构的B+树能够存储更多的索引。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://www.youtube.com/watch?v=aZjYr87r1b8" target="_blank" rel="noopener">YouTube 视频</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B树/B+树 </tag>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习心得 - TCP协议</title>
      <link href="/2019/12/26/2019-12-26-tcp/"/>
      <url>/2019/12/26/2019-12-26-tcp/</url>
      
        <content type="html"><![CDATA[<h2 id="0-写在前面"><a href="#0-写在前面" class="headerlink" title="0. 写在前面"></a>0. 写在前面</h2><p>这学期上的唯二两门CS课之一就是<a href="https://computer-networks.github.io/fa19/index.html" target="_blank" rel="noopener">15641 Computer Network</a>，在这之前对于网络的全部理解大概就是“重启路由器”了。在上课之前我一直认为网络这个东西相对独立，即大多数的工作都不需要计算机网络的相关知识。但是上完课之后我觉得这样的认识是肤浅的，网络中的很多概念和想法其实是很通用的，例如一个CDN就可以看成是一个巨大的NUMA machine；一个LAN也可以看成是一个放大版的CPU多核通信。</p><h2 id="1-TCP概述"><a href="#1-TCP概述" class="headerlink" title="1. TCP概述"></a>1. TCP概述</h2><p>首先TCP，即<em>Transmission Control Protocol</em>，传输控制协议。顾名思义，它的主要任务就是进行数据的<strong>传输</strong>，<strong>控制</strong>也是为了<strong>更好</strong>的实现这个传输任务。</p><p>以上加粗的三个词即构成了整个TCP的核心。即以什么形式传输？如何开始or结束传输？为什么需要控制？需要什么样的控制？什么叫做“更好”？</p><h2 id="2-传输"><a href="#2-传输" class="headerlink" title="2. 传输"></a>2. 传输</h2><h3 id="2-1-以什么形式传输？"><a href="#2-1-以什么形式传输？" class="headerlink" title="2.1 以什么形式传输？"></a>2.1 以什么形式传输？</h3><p><strong>Packet</strong>，即（数据）包。相信大家如果接触过网络相关的知识对这个词一定不陌生，通常所说的抓包、丢包的“包”指的就是这个Packet。直接讲这个packet可能会有些突兀，首先要问的问题是，为什么要用这个packet？有没有一些别的方式？</p><p>这里要稍微提一些历史，在网络发明以前，能够做到大范围通信的是什么？</p><p>——有线电话。</p><p>在一些怀旧影片中大家或许会注意到有这样一个职位：接线员（如下图）。以前打电话的时候通常都是先打到一个专接中心，在你告知接线员需要转接哪里后他就会帮你进行人为的接线操作。</p><p><img src="/images/image-20191226155656485.png" alt="有线电话的接线机制"></p><p>这当然是网络传输方式的一种选择，只是这种选择的弊端太明显，即成本高、不能规模化。考虑到如今网络的两大特点，用户群庞大以及流量分散，这种方式是注定不可行的。当然它也有它的好处，即带宽是稳定的。</p><p>至此，大家就把目光投向了另一种大范围通信的系统——邮件系统。它的弊端是慢，以前靠人力、畜力，现在靠内燃机，还是慢。但是在互联网的设计中，这种弊端是全然不存在的，模拟信号在电缆中的传输速度接近光速。</p><p>由此互联网便建立起了类似于邮件系统的数据传输模式——数据包收发（Packet switching）。它可以复用线路、分布式部署并且不需要人力进行管控，一旦部署完成即可自动收发数据包。其中最重要的一个特性就是对线路的复用（statistical multiplexing），这直接降低了分摊到每个用户的成本，同时提高了线路的效用。</p><p>注：关于数据包的基本信息以及如何进行收发等可见IP相关知识。通常所说的数据包即指的是IP层可见的部分。</p><h3 id="2-2-如何开始传输"><a href="#2-2-如何开始传输" class="headerlink" title="2.2 如何开始传输"></a>2.2 如何开始传输</h3><p>IP层的主要目的以及用途即尽可能的将数据包从A发送到B，而且对于每个数据包都是独立的，即IP层不会管数据包的内容尤其是先后顺序，而数据的有序性是两个客户端进行通信的很重要的一方面，这也正是TCP的主要特点之一（UDP则不保证有序性）。</p><h4 id="2-2-1-为什么不直接传输数据"><a href="#2-2-1-为什么不直接传输数据" class="headerlink" title="2.2.1 为什么不直接传输数据"></a>2.2.1 为什么不直接传输数据</h4><p>所谓通信，即两个客户端开始互相有目的性得传输特定的数据。那么第一步，如何开始这个通信？通常这两个客户端中有一个为发起者（initiator），本次通信的第一个包即由他来发起。需要注意的是，这第一个包并不是真正的数据包，他更像是一个用来打招呼的包，我们称之为SYN（即指代Synchronize，同步），为什么需要有一个打招呼的包？最主要的原因是通信双方需要互相交换（同步）各自的数据序号（sequence number）。数据序号，顾名思义就是对每一个自己发出的数据（以字节为单位）编号。这个序号正是TCP用以保证数据的有序性的关键。</p><p>注：为什么需要双方互相同步呢？在这里完全取决于应用层（Application layer）的选择，如果是类似HTTP的应用，则显然双方都有可能发送数据，但如果是明确只需要一方发送数据的，则改用UDP即可。</p><h4 id="2-2-2-如何打招呼——三次握手"><a href="#2-2-2-如何打招呼——三次握手" class="headerlink" title="2.2.2 如何打招呼——三次握手"></a>2.2.2 如何打招呼——三次握手</h4><p>“三次握手”这个词似乎已经烂大街了，为什么是三次呢？我们先来讲，什么是一个完整的“招呼”。前面说到的，在这个过程中，双方需要互相交换（同步）各自的（初始）数据序号。最直接的想法就是：</p><ol><li><p>A发送自己的初始序号给B</p></li><li><p>B做出回应表示收到</p></li><li><p>B发送自己的初始序号给</p></li><li><p>A做出回应表示收到</p></li></ol><p>这样是四次握手（一次握手可以理解为一个数据包），了解数据包的结构就不难发现，以上四个数据包中，2和3是可以合并的。于是就简化为了熟知的三次握手。在结束通信阶段，也有对应的握手，三次和四次均可（因为通信开始可以认为是一起开始的，但是结束则不一定）。</p><h2 id="3-控制"><a href="#3-控制" class="headerlink" title="3. 控制"></a>3. 控制</h2><h3 id="3-1-为什么需要控制"><a href="#3-1-为什么需要控制" class="headerlink" title="3.1 为什么需要控制"></a>3.1 为什么需要控制</h3><p>在这里我们主要是站在数据发送方的视角考虑，当你在发送数据时，自然是希望越快越好，但是有以下两方面的考虑</p><ol><li>接收方的存储空间是有限的，特别的，为某一次传输准备的缓存空间更是有限的</li><li>网络传输是受带宽限制的，特别的，路由器的缓存队列也是有限的</li></ol><p>考虑到第一点，如果你的发送速率过快，则会快速占满接收方的缓存空间，在之后你发送的包都会被丢弃，这显然是不可取的。</p><p>考虑到第二点，如果你的发送速率过快，则会快速占满路由器的缓存队列（这里的路由器可以是你的包所经过的任意一个，这里可以仅关注离接收方最近的那个路由器），与此同时，其他的数据发送方发出的包就会被丢弃，这对于其他的接收/发送方是不公平的，并且如果大家都这样做，那就形成了恶性循环，也显然是不可取的。</p><h3 id="3-2-需要什么样的控制"><a href="#3-2-需要什么样的控制" class="headerlink" title="3.2 需要什么样的控制"></a>3.2 需要什么样的控制</h3><ol><li>针对上述的第一个考虑，TCP里面就引入了流量控制（flow control）的概念。具体的实现方式是让接收方在每次发送的ACK数据包内包含接收方剩余buffer大小的信息，这个数据包含在TCP头信息内。这样，发送方就可以根据这个信息，对自身的滑动窗口大小做一个上限的限制。</li><li>针对上述第二个考虑，TCP引入了拥塞控制（congestion control）的概念。具体的实现方式是根据收到的ACK数据包来进行对滑动窗口大小的动态调整。这里的具体实现方式有多种，例如经典的Reno，Cubic以及谷歌推出的BBR。</li></ol><h3 id="4-TCP的状态机"><a href="#4-TCP的状态机" class="headerlink" title="4. TCP的状态机"></a>4. TCP的状态机</h3><p><img src="/images/image-20200629215506560.png" alt="TCP状态转移图"></p><p><a href="https://book.systemsapproach.org/index.html#" target="_blank" rel="noopener">Computer Networks: A Systems Approach</a> 中的这张状态转移图是十分明了直观的。其中值得关注的一点是在结束阶段，先结束（先发FIN）的一方需要在对另一方的FIN作出ACK后等待一段时间（<code>TIME_WAIT</code>状态）。之所以这样做是因为，作为先结束的一方，最后收到另一方的FIN并作出ACK后，无法保证该ACK包被另一方顺利收到（例如丢包等）。而如果此时默认对方收到并结束连接的话，很有可能会在之后收到另一方重发的FIN包。这个FIN包可能会导致之后的由同一个端口开启的TCP连接提前结束。</p><p>如果没有等待时间，一个可能的事件流如下</p><ol><li>A向B发送FIN</li><li>B回复ACK到A</li><li>A收到B的FIN</li><li>A回复ACK给B，并直接变成结束状态</li><li>B在timeout之后重发FIN</li><li>B收到了之前A回复的ACK包，B变成结束状态</li><li>B又在同一个端口与A建立了连接，经过握手阶段双方进入<code>Established</code>状态</li><li>A收到之前一次B重发的FIN包，A回复ACK后进入<code>CLOSE_WAIT</code>状态</li><li>在此次连接中A并无数据发送，发送FIN并进入<code>LAST_ACK</code>状态</li><li>B对A的FIN回复ACK，A收到后就进入了<code>CLOSED</code>状态</li></ol><p>这样就因为没有预留等待时间而导致下次一的连接提前结束。这也正是IP层的魔力所在，即不确定性。数据包到达顺序不确定，到达时间不确定。因此，所有建立在IP层之上的协议都需要充分考虑到这一点，并根据自身需求作出相应的机制来避免与预期不相符的表现。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] 课程 <a href="https://computer-networks.github.io/fa19/index.html" target="_blank" rel="noopener">CMU 15-641 lecture</a></p><p>[2] 书本 <a href="https://book.systemsapproach.org/index.html#" target="_blank" rel="noopener">Computer Networks: A Systems Approach</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习心得 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TCP协议 </tag>
            
            <tag> 计算机网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记 - Identity Mappings in Deep Residual Networks</title>
      <link href="/2018/08/30/2018-08-30-paper-note-identity-mappings-in-deep-residual-networks/"/>
      <url>/2018/08/30/2018-08-30-paper-note-identity-mappings-in-deep-residual-networks/</url>
      
        <content type="html"><![CDATA[<p><a href="https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38" target="_blank" rel="noopener">Original Paper</a></p><h2 id="1-论文概述"><a href="#1-论文概述" class="headerlink" title="1. 论文概述"></a>1. 论文概述</h2><ul><li>本文主要探讨在残差网络[1]一文中所使用的单位链接（Identity Mapping）在网络中的前向和反向传播性质</li></ul><h2 id="2-具体分析"><a href="#2-具体分析" class="headerlink" title="2. 具体分析"></a>2. 具体分析</h2><h3 id="2-1-前向和反向传播性质"><a href="#2-1-前向和反向传播性质" class="headerlink" title="2.1 前向和反向传播性质"></a>2.1 前向和反向传播性质</h3><ul><li><p>在[1]中的残差结构主要为<br><img src="/images/8CFA680D495BE9D378D8B627F9CAB89B.jpg" alt=""><br>其中 $x_l, x_{l+1}$ 分别为第 $l$ 和 $l+1$ 层的输入，$f$ 是第 $l$ 层的激活函数，$h$ 则为单位映射即 $h(x) = x$，$F$ 为所需学习的残差函数。如果我们不妨简单得设激活函数 $f$  亦为单位映射，则我们可以得到如下前向递推关系：<br><img src="/images/C0E278A057DF75F59F06507746409137.jpg" alt=""><br>公式（4）有两点很好的（前向传播）性质：</p><ul><li>其中第二项加和项亦可以看作是第 $l$ 层和第 $L$   层之间的残差（这一点十分优雅，这样的话堆叠残差块就没有看上去那么粗暴了）</li><li>第 $L$ 层的输入就等价于前面 $L-l$ 个残差模块的输出之和再加上第 $l$ 层的原始输入，这就区别于传统的神经网络（可以看作是 $L-l$ 个参数矩阵与原始输入的乘法）</li></ul><p>公式（4）对于反向传播也有很好的性质：</p><ul><li>记误差函数为大epsilon（这里打不出，记为E），则有反向传播如下：<br><img src="/images/A34661A9A61D9653F6481A9B075DA477.jpg" alt=""><br>上式将梯度 $\frac{\partial E}{\partial x_l}$ 分解为两项之和，其中第一项为第 $L$ 层输入的直接梯度，第二项是中间各层的梯度之和，而前面一项正是保证了第 $L$ 层的梯度能够直接传到任意浅的第 $l$ 层</li><li>这样的一个梯度分解也可以很大程度上缓解梯度逐层传播而消失的问题，缘于分解中的后一项 $\frac{\partial}{\partial x_L}\sum_{i=l}^{L-1}F(x_i,W_i)$ 不可能在一个小批量中对所有的训练样本都等于 $-1$（这个可能有点主观）</li></ul></li><li><p>注：公式（4）的前提是</p><ul><li>单位链接</li><li>单位激活<br>故接下来将单独分析这两个前提对模型的影响。</li></ul></li></ul><h3 id="2-2-单位链接的重要性"><a href="#2-2-单位链接的重要性" class="headerlink" title="2.2 单位链接的重要性"></a>2.2 单位链接的重要性</h3><ul><li>考虑另一种简单的情形：<br><img src="/images/9ED8EAB4C0CE4C297EA13012FA743A20.jpg" alt=""><br>在这里我们仍然假设激活函数是单位激活函数，$f(x) = x$。那么公式（4）就相应地变为：<br><img src="/images/246B5B61E74BD075347B975ABCA06E83.jpg" alt=""><br>同样地公式（5）就变为：<br><img src="/images/7E8417DE1A7A376F5C338E1996D4C3AD.jpg" alt=""><br>那么可以看出，如若第一项 $\prod_{i=1}^{L-1}\lambda_i$ 中的 ${\lambda_i}$ 有很多项或者全部大于 $1$，又或者小于 $1$，当网络越来越深， $L-l$越来越大的时候，则会使得这一项变得很大亦或者变得很小，接近于零而消失，都会使得模型的学习出现障碍，造成模型优化困难。这一点由实验佐证（具体见论文）<ul><li>注：<br>  类似的跨层链接（shortcut connection）是一种信息传播很直接的方法，但是在其中加入一些复杂的操作例如（scaling,gating,1x1 convolution and dropout）会导致优化问题——即虽然增加了模型的表达能力（representation power）但是却很难优化模型，使得最后的结果反倒不如单位链接。</li></ul></li></ul><h3 id="2-3-激活函数的选择（跨层链接的结构）"><a href="#2-3-激活函数的选择（跨层链接的结构）" class="headerlink" title="2.3 激活函数的选择（跨层链接的结构）"></a>2.3 激活函数的选择（跨层链接的结构）</h3><p>下图是所有我们进行分析的一些跨层链接的结构<br><img src="/images/4C30828771694C85B58BF71BA4C4818E.jpg" alt=""><br>其中（a）是[1]中的默认设置。并且这里需要说明一点的是，如果激活函数使用了ReLU（也是实验中真的用到的，见（a）），那么之前那些公式的理论推导就相当于是一种近似。接下来我们主要探讨图（4）中的一些残差模块结构的变体。</p><ul><li>（b）BN after addition<br>这个表现很差，又回到了没有引入残差网络时的情况了</li><li>（c）ReLU before addition<br>虽然这么做激活函数 $f$ 确实变成了单位映射，然而由于ReLU本身的非负性导致该情形下的残差是非负的，这与残差本身的意义不符。实验也表明了此变体的表现有所下降</li><li>（d）Pre-activation or Post-activation？<br>在原始结构（a）中，激活函数 $f$ 不仅会影响下一个残差模块的输入，同时也影响到了下一个模块的跨层链接，也就是指：<br><img src="/images/5B6BD8330208A2EC7BB7537B93A5F661.jpg" alt="">{:height=”50%” width=”50%”}<br>而（d）变体结构正是想要将这双重影响区分开来，从而变为：<br><img src="/images/DFFA3A566BDF9754A5CC5ADF6184DD72.jpg" alt=""><br>此时那激活函数 $f$ 也变成单位映射了。其实这里所讲的 <em>pre-activation</em> 或者是 <em>Post-activation</em> 指的是激活函数（ReLU或BN）相对于目标参数是先作用还是后作用。例如图（4）中（d）结构，ReLU是先作用于weight；（e）结构，ReLU和BN都是先作用于weight，这一点需区分于（a，b，c）结构。最后实验结果表明，（e）结构，当两者都先作用时表现最好。</li></ul><h3 id="2-4-进一步分析结构（e）full-pre-activation"><a href="#2-4-进一步分析结构（e）full-pre-activation" class="headerlink" title="2.4 进一步分析结构（e）full pre-activation"></a>2.4 进一步分析结构（e）full pre-activation</h3><p>其带来的影响主要有两点：</p><ul><li><p>此时的跨层链接是单位映射，故训练过程会更加容易</p></li><li><p>优先使用BatchNorm能够对网络起到正则化的效果</p></li></ul><p><em>这一部分主要看实验结果</em>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1]  <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 残差网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记 - Deep Residual Learning for Image Recognition</title>
      <link href="/2018/08/28/2018-08-28-paper-notes-deep-residual-learning-for-image-recognition/"/>
      <url>/2018/08/28/2018-08-28-paper-notes-deep-residual-learning-for-image-recognition/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" target="_blank" rel="noopener">Original Paper</a></p><h2 id="1-论文概述"><a href="#1-论文概述" class="headerlink" title="1. 论文概述"></a>1. 论文概述</h2><ul><li>主要面对的问题<ul><li>深度学习模型（神经网络）越深，越难训练</li></ul></li><li>解决方案<ul><li>提出了残差网络（residual learning framework），将学习‘未知函数’的任务转化为学习与输入的残差函数</li><li>实验表明残差网络更易训练，并且能够得益于更深的网络结构</li></ul></li></ul><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><ul><li>残差表示（Residual Representations）<ul><li>These methods suggest that a good reformulation or preconditioning can simplify the optimization.</li></ul></li><li>跨层链接（Shortcut Connections）<ul><li>在残差结构（residual block）中，这种链接是单位链接（Identity shortcut connection），也就意味着没有参数；另一方面，如果将这部分链接的量置为0，该层网络结构也就与正常网络结构无异</li></ul></li></ul><h2 id="3-深度残差学习"><a href="#3-深度残差学习" class="headerlink" title="3. 深度残差学习"></a>3. 深度残差学习</h2><h3 id="3-1-残差学习"><a href="#3-1-残差学习" class="headerlink" title="3.1 残差学习"></a>3.1 残差学习</h3><p>我们记 $H(x)$ 为所需学习的函数，其由一些连续的网络层（可以不是整个网络）所代表；$x$ 记为这些层最开始的输入。如果假设这些网络层能够模拟复杂函数 $H(x)$，那么同样地，我们也可以假设其可以模拟残差函数 $H(x)-x$，记为 $F(x)$，于是原函数就变为 $F(x)+x$。</p><ul><li>动机<br><img src="/images/3EDE3DAB09BAACEA31F6EEE3FCA0E463.jpg" alt=""><br>如图（1）左所示，更深的神经网络的训练误差反而明显高于较浅的神经网络，这是非常反直觉的——即使更深的网络层都学到的是单位映射，也应该与较浅的网络结构表现相同，也不至于差差这么多——这就暗示了多层非线性神经网络可能在学习单位映射时很困难。</li></ul><p>现在有了残差网络这种重新的表示形式（reformulation），较深的层只需要将网络参数都置为0，即可学到单位映射（因为还有残差链接）。当然，在实际问题中很少有需要网络学习单位映射的情形，但是我们的残差网络可以帮助重新定义问题（即学习的目标函数改变了）。</p><ul><li>残差模块（residual block）<br>最典型的残差模块如下图所示：<br><img src="/images/C7A2C8F7F15C1AADF5E47CB98F983CAC.jpg" alt=""><br>输入和输出定义为：</li></ul><p>$$<br>  y = \mathcal{F}(x, {W_i}) + x<br>$$</p><p>  这里的“+”即为对应元素之和（参考矩阵加法）；并且这里潜在要求 $y$ 和 $x$ 具有相同的纬度和尺寸，否则可以用一个线性映射来进一步调整残差的纬度和尺寸以符合输出：</p><p>$$<br>  y = \mathcal{F}(x, {W_i}) + W_sx<br>$$</p><p>  注：此处 $W_s$ 当然可以是一个矩阵作为残差网络的一组参数进行学习，但是实验表明单位映射已经表现得足够好并且又十分高效简洁。</p>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
            <tag> 残差网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记 - Batch Normalization</title>
      <link href="/2018/08/20/2018-08-20-paper-notes-batch-normalization/"/>
      <url>/2018/08/20/2018-08-20-paper-notes-batch-normalization/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1502.03167.pdf%E7%9A%84paper%E9%80%82%E5%90%88%E6%83%B3%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3%E5%8E%9F%E7%90%86%EF%BC%8C%E8%BF%99%E4%B8%AA%E8%A7%86%E9%A2%91%E5%BE%88%E6%B8%85%E6%A5%9A%E7%9A%84%E8%AE%B2%E4%BA%86bn%E8%B5%B7%E5%88%B0%E7%9A%84%E4%BD%9C%E7%94%A8%E3%80%82" target="_blank" rel="noopener">Original Paper</a></p><h2 id="0-Terms"><a href="#0-Terms" class="headerlink" title="0. Terms"></a>0. Terms</h2><ul><li>Saturating non-linearities<ul><li>Intuitions:<br>A saturating activation function squeezes the input.</li><li>Definitions:<br><img src="/images/C23B797B65281C8DF41F713178116411.jpg" alt=""></li><li>Examples:<ul><li>Rectified Linear Unit (ReLU) activation function is <strong>non-saturating</strong></li><li>Sigmoid activation function is <strong>saturating</strong></li><li>Hyperbolic tanget function（tanh）is <strong>saturating</strong></li></ul></li></ul></li><li>Internal Covariate Shift<ul><li>the change in the distribution of network activations due to the change in network parameters during training</li></ul></li></ul><h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><ul><li>Phenomenon: <em>internal covariate shift</em><ul><li>classical NN training is slow and hard to train with saturating non-linearities</li></ul></li><li>Solution:  making normalization a part of the model architecture and performing the normalization for each training mini-batch</li><li>Advantages:<ul><li>higher learning rates（train faster）</li><li>be less dependent on parameter initialization</li><li>it can also replace the role of Dropout, in some cases</li></ul></li></ul><h2 id="2-Why"><a href="#2-Why" class="headerlink" title="2. Why"></a>2. Why</h2><h3 id="2-1-Why-mini-batch"><a href="#2-1-Why-mini-batch" class="headerlink" title="2.1 Why mini-batch"></a>2.1 Why mini-batch</h3><ul><li>mini-batch gradient is more robust</li><li>mini-batch calculation is more efficient due to parallelism （i.e, GPU）</li></ul><h3 id="2-2-Why（internal）covariate-shift"><a href="#2-2-Why（internal）covariate-shift" class="headerlink" title="2.2 Why（internal）covariate shift"></a>2.2 Why（internal）covariate shift</h3><ul><li><p>Small changes to the network parameters amplify as the network becomes deeper（forward feeding）</p></li><li><p>When ‘the input distribution to a learning system’ changes, it is said to experience covariate shift（Shimodaira, 2000）</p></li><li><p>Consider such a network computing:</p></li></ul><p>$$<br>  l = F_2(F_1(u, \Theta_1), \Theta_2)<br>$$</p><p>  the former output serves as the input of the latter network, and it’s <strong>equivalent</strong> that the output comes from other place rather than the former layer. So it is <strong>advantageous</strong>（don’t understand well）for the distribution of the input of latter layer to remain fixed over time.</p><ul><li><strong>A critical point</strong><ul><li>Saturate non-linear activation functions, such as <em>sigmoid</em>, will suffer <em>gradient vanishing</em> if the <em>norm</em> of input gets too big.</li><li>This effect is amplified as the network depth increases.</li><li>In practice, this problem is avoided by using non-saturate functions like <em>ReLU</em> as well as small learning rates.</li><li>So, <strong>what if we could control the norm of the input and make its distribution more stable?</strong></li></ul></li></ul><h3 id="2-3-Why-Batch-Normalization"><a href="#2-3-Why-Batch-Normalization" class="headerlink" title="2.3 Why Batch Normalization"></a>2.3 Why Batch Normalization</h3><ul><li>We achieve such ‘control’ by introducing the new mechanism – Batch Normalization, which reduces the <em>internal covariate shift</em> and accelerates network training.</li><li>It has been long known（LeCun et al., 1998b; Wiesler &amp; Ney, 2011）that the network training <strong>converges faster</strong> if its inputs are whitened（closer to white noise）. i.e., linearly transformed to have zero means and unit variances, and decorrelated.</li><li>Furthermore, batch normalization regularizes the model and reduces the need for Dropout（Srivastava et al., 2014）.</li></ul><h3 id="2-4-Two-problems-of-‘Gradient-Descent-Normalization’"><a href="#2-4-Two-problems-of-‘Gradient-Descent-Normalization’" class="headerlink" title="2.4 Two problems of ‘Gradient Descent + Normalization’"></a>2.4 Two problems of ‘Gradient Descent + Normalization’</h3><ul><li>Consider whitening activations at every training step, interspersed with gradient descent. The combination of update to <em>b</em>, a learned bias, and subsequent change through normalization will lead to no change in the output and, also, the loss function w.r.t the bias term, <em>b</em>. Thus, <em>b</em> will grow indefinitely while the loss remains ‘fixed’ w.r.t <em>b</em>.</li><li>Consider a deep NN with single unit at each layer and the forward equation is $y = x\prod_{i=1}^l w_i$. After back propagation, we have $y’ = x\prod_{i=1}^l (w_i - \epsilon g_i)$. For instance, the term $\epsilon^2g_1g_2\prod_{i=3}^l w_i$ will goes to infinity if $l$ is big（deep network）and ${w_i}_{i\ge 3}$ are bigger than $1$. This will lead to update unstability and gradient vanishing for saturating non-linear activation functions.</li><li><strong>Main issue with the above problems:</strong> the gradient descent procedure doesn’t take into account the normalization process.</li><li><strong>To address this issue</strong>, one should ensure that the network always produces activations with the desired distribution. (output -&gt; BN -&gt; activation)</li></ul><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><h3 id="3-1-Why-not-full-whitening-of-each-layer’s-input"><a href="#3-1-Why-not-full-whitening-of-each-layer’s-input" class="headerlink" title="3.1 Why not full whitening of each layer’s input?"></a>3.1 Why not full whitening of each layer’s input?</h3><ul><li>Computationally expensive<br>For back propagation, we need to compute the Jacobians:</li></ul><p>$$<br>  \frac{\partial \text{Norm}(x,\chi)}{\partial x} \text{ and }\frac{\partial \text{Norm}(x,\chi)}{\partial \chi}<br>$$</p><p>  Ignoring the second term will lead to the first problem mentioned above. And, the second term requires the covariance matrix of the design matrix $\chi$, along with its square root matrix. However, for deep networks, the training set $\chi$ is usually quite large.</p><ul><li>Not everywhere differentiable</li></ul><h3 id="3-2-Normalization-via-Mini-Batch-Statistics"><a href="#3-2-Normalization-via-Mini-Batch-Statistics" class="headerlink" title="3.2 Normalization via Mini-Batch Statistics"></a>3.2 Normalization via Mini-Batch Statistics</h3><ul><li><p>Two simplifications:</p><ul><li><p>Instead of whitening the features in layer inputs and outputs jointly, we will normalize each scalar feature <strong>independently</strong>, by making it have the mean of zero and the variance of 1</p><ul><li><p>To ensure the representation power of certain layer, we introduce, for each activation $x^{(k)}$, a pair of parameters $\gamma^{(k)}, \beta^{(k)}$, which scale and shift the normalized value:</p><p>$$<br>y^{(k)} = \gamma^{(k)}\widehat{x}^{(k)} + \beta^{(k)}<br>$$</p><p>The two parameters are updated through the same way as other network parameters.</p></li></ul></li><li><p>Since we use mini-batches in stochastic gradient training, each mini-batch produces <strong>estimations</strong> of the mean and variance of each activation</p></li></ul></li><li><p>The Batch Normalization Algorithm<br><img src="/images/C42D1A69A10B8BA341F4A73A1D088296.jpg" alt=""></p></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean" target="_blank" rel="noopener"><a href="https://stats.stackexchange.com/questions/174295/what-does-the-term-saturating-nonlinearities-mean" target="_blank" rel="noopener">What does the term saturating nonlinearities mean?</a></a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Batch Norm </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
